# -*- coding: utf-8 -*-
"""LungInfSegm.ipynb

Segmentation Models
https://github.com/qubvel/segmentation_models
https://segmentation-models.readthedocs.io/en/latest/tutorial.html


Reducing LR on Plateau
https://keras.io/api/callbacks/reduce_lr_on_plateau/

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/asandino87/LungCT/blob/v4.8/LungInfSegm.ipynb
    
https://stackoverflow.com/questions/67792138/attributeerror-module-keras-utils-has-no-attribute-get-file-using-segmentat
    
"""
#%%
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras import Input,layers, models
from tensorflow.keras.layers import Conv2DTranspose,Dropout,Conv2D,BatchNormalization, Activation,MaxPooling2D
from tensorflow.keras import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler

import math
import albumentations as A
import matplotlib.pyplot as plt

import tensorflow as tf
#tf.test.gpu_device_name()

#%%
"""**Experimental setup**"""

experiment_name = 'LungInf_SF2_Filt64_25052021'
scale_factor = 8
filters = 2
image_size = 2048

input_dir = '/home/usuario/Documentos/GBM/Samples/PC_SegmentationSet/train2/PC/'
mask_dir = '/home/usuario/Documentos/GBM/Samples/PC_SegmentationSet/train2/Mask/'

val_dir = '/home/usuario/Documentos/GBM/Samples/PC_SegmentationSet/val/PC/'
mask_val_dir = '/home/usuario/Documentos/GBM/Samples/PC_SegmentationSet/val/Mask/'

target_size=(image_size//scale_factor, image_size//scale_factor)

image_datagen = ImageDataGenerator(
                                  rescale=1./255,
                                  rotation_range=20,                                  
                                  fill_mode='reflect',
                                  zoom_range=0.2,
                                  #width_shift_range=0.01,
                                  #height_shift_range=0.01,
                                  horizontal_flip=True,
                                  vertical_flip=True,
                                  #channel_shift_range=0.1,
                                  )

# mask_datagen = ImageDataGenerator(rescale=1./255)

image_datagen_val = ImageDataGenerator(rescale=1./255)
mask_datagen_val = ImageDataGenerator(rescale=1./255)

image_generator = image_datagen.flow_from_directory(
    input_dir,
    class_mode=None, target_size=target_size,  batch_size=4,
    seed=1)

mask_generator = image_datagen.flow_from_directory(
    mask_dir,
    class_mode=None, target_size=target_size, batch_size=4,
    seed=1)

image_generator_val = image_datagen_val.flow_from_directory(
    val_dir,
    class_mode=None, target_size=target_size, batch_size=4,
    seed=1)

mask_generator_val = mask_datagen_val.flow_from_directory(
    mask_val_dir,
    class_mode=None, target_size=target_size, batch_size=4,
    seed=1)

#%%
steps = image_generator.n//image_generator.batch_size
steps_val = image_generator_val.n//image_generator_val.batch_size


train_generator = zip(image_generator, mask_generator)
val_generator = zip(image_generator_val, mask_generator_val)

for _ in range(1):
    img = image_generator.next()
    mask = mask_generator.next()
    
    print(img.shape)
    plt.figure(1)
    plt.subplot(1,2,1)
    plt.axis('off')
    plt.imshow(img[0])
    plt.subplot(1,2,2)
    plt.imshow(mask[0])
    plt.axis('off')
    plt.show()
    
#%%

import segmentation_models as sm
from segmentation_models import Unet
from segmentation_models import Linknet
from segmentation_models import FPN
# from segmentation_models.utils import set_trainable

#%%

sm.set_framework('tf.keras')

sm.framework()

# model = Unet(backbone_name='resnet50', encoder_weights=None, encoder_freeze=False)

model = Unet(backbone_name='efficientnetb4', 
              input_shape=(target_size[0], target_size[1], 3),
             encoder_weights=None, 
             classes=1, 
             activation='sigmoid')
             # encoder_freeze=False)

# BACKBONE = 'resnext50'
# preprocess_input = sm.get_preprocessing(BACKBONE)


# # define model
# model = sm.Unet(BACKBONE, encoder_weights=None)

model.summary()

#%%
"""
***Callbacks***

A callback is an object that can perform actions at various stages of training 
(e.g. at the start or end of an epoch, before or after a single batch, etc).

https://keras.io/api/callbacks/
"""
from tensorflow.keras.callbacks import ReduceLROnPlateau

# Reduce LR on Plateau
RoP = ReduceLROnPlateau(monitor="val_loss",
                        factor=0.1,
                        patience= 100,
                        min_lr=1e-5,)

# Step LR decay
def step_decay(epoch):
	initial_lrate = 1e-3
	drop = 0.1
	epochs_drop = 50
	lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
	return lrate

lr = LearningRateScheduler(step_decay)

# Early Stopping
es = EarlyStopping(patience=1000,mode='min', verbose=1)

#%%

# Model Checkpoint
checkpoint_path ='/home/usuario/Documentos/GBM/Experimentos/PseudPSegmModel_new.h5'
mc = ModelCheckpoint(checkpoint_path, 
                     monitor='val_loss', 
                     verbose=1 , 
                     save_best_only=True, 
                     mode='min')

model.compile(optimizer='adam',
              loss=sm.losses.bce_jaccard_loss,
              metrics=[sm.metrics.iou_score],
              )

#%%
history = model.fit(train_generator,
                    steps_per_epoch=steps,
                    validation_data=val_generator,
                    validation_steps=steps_val,
                    epochs=2000,
                    verbose=1,
                    callbacks=[es,mc,RoP]
                    
                    )

#%%
"""
def conv_block(tensor, nfilters, size=3, padding='same', initializer="he_normal"):
    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(tensor)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    return x


def deconv_block(tensor, residual, nfilters, size=3, padding='same', strides=(2, 2)):
    y = Conv2DTranspose(nfilters, kernel_size=(size, size), strides=strides, padding=padding)(tensor)
    y = tf.concat([y, residual], axis=3)
    y = conv_block(y, nfilters)
    return y


def Unet(img_height, img_width, nclasses, filters):
# down
    input_layer = Input(shape=(img_height, img_width, 3), name='image_input')
    conv1 = conv_block(input_layer, nfilters=filters)
    conv1_out = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = conv_block(conv1_out, nfilters=filters*2)
    conv2_out = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = conv_block(conv2_out, nfilters=filters*4)
    conv3_out = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = conv_block(conv3_out, nfilters=filters*8)
    conv4_out = MaxPooling2D(pool_size=(2, 2))(conv4)
    conv4_out = Dropout(0.5)(conv4_out)
    conv5 = conv_block(conv4_out, nfilters=filters*16)
    conv5 = Dropout(0.5,name='BOTTLENECK')(conv5)
# up
    deconv6 = deconv_block(conv5, residual=conv4, nfilters=filters*8)
    deconv6 = Dropout(0.5)(deconv6)
    deconv7 = deconv_block(deconv6, residual=conv3, nfilters=filters*4)
    deconv7 = Dropout(0.5)(deconv7) 
    deconv8 = deconv_block(deconv7, residual=conv2, nfilters=filters*2)
    deconv9 = deconv_block(deconv8, residual=conv1, nfilters=filters)
# output
    output_layer = Conv2D(filters=1, kernel_size=(1, 1))(deconv9)
    output_layer = BatchNormalization()(output_layer)
    output_layer = Activation('sigmoid')(output_layer)

    model = Model(inputs=input_layer, outputs=output_layer, name='Unet')
    return model

nclasses=2

model = Unet(image_size//scale_factor, image_size//scale_factor, nclasses, filters)

model.summary()

#%%

    
def soft_dice(y_pred, y_true):
    import keras.backend as T
    # y_pred is softmax output of shape (num_samples, num_classes)
    # y_true is one hot encoding of target (shape= (num_samples, num_classes))
    #a=0
    intersect = T.sum(y_pred * y_true, 0)
    denominator = T.sum(y_pred, 0) + T.sum(y_true, 0)
    dice_scores = T.constant(1)-T.constant(2) * intersect / (denominator + T.constant(1e-6))
    #print("aaa")
    return dice_scores

def soft_dice_metrics(y_pred, y_true):
    import keras.backend as T
    # y_pred is softmax output of shape (num_samples, num_classes)
    # y_true is one hot encoding of target (shape= (num_samples, num_classes))
    #a=0
    intersect = T.sum(y_pred * y_true, 0)
    denominator = T.sum(y_pred, 0) + T.sum(y_true, 0)
    dice_scores = T.constant(2) * intersect / (denominator + T.constant(1e-6))
    #print("aaa")
    return dice_scores




def step_decay(epoch):
	initial_lrate = 1e-3
	drop = 0.1
	epochs_drop = 100
	lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
	return lrate

lr = LearningRateScheduler(step_decay)
es = EarlyStopping(patience=1000,mode='min', verbose=1)
checkpoint_path ='/home/usuario/Documentos/GBM/Experimentos/PseudPSegmModel.h5'

mc = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1 , save_best_only=True, mode='min')

def create_keras_model():
    
    model2 = model
    
    model2.compile(optimizer='adam',
                  #loss=IoULoss,
                  loss = soft_dice,
                  metrics=['accuracy',soft_dice_metrics]
                  )

    return model2

# #%%
# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
# # create the classifier
# classifier = KerasClassifier(create_keras_model)

# #%%


# from modAL.models import ActiveLearner

# # initialize ActiveLearner
# learner = ActiveLearner(
#     estimator=classifier,
#     X_training=image_generator_val[0][0] , y_training=np.ndarray.flatten(mask_generator_val[0][0]),
#     verbose=1
# )


#%%

model.compile(optimizer='adam',
              #loss=IoULoss,
              loss = soft_dice,
              metrics=['accuracy',soft_dice_metrics]
              )

#%%

history = model.fit(train_generator,
                    steps_per_epoch=steps,
                    validation_data=val_generator,
                    validation_steps=steps_val,
                    epochs=50,
                    verbose=1,
                    
                    )
"""
#%%

classifier = KerasClassifier(create_keras_model)

#%%
learner = ActiveLearner(
    estimator=classifier,
    X_training=X_initial, y_training=y_initial,
    verbose=1
)


#%%
# learning rate schedule

# def step_decay(epoch):
# 	initial_lrate = 1e-3
# 	drop = 0.1
# 	epochs_drop = 50
# 	lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
# 	return lrate

# lr = LearningRateScheduler(step_decay)
# es = EarlyStopping(patience=20,mode='min', verbose=1)
# checkpoint_path ='/home/usuario/Documentos/GBM/Experimentos/PCSegmModel.h5'

# mc = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1 , save_best_only=True, mode='min')

# model.compile(optimizer='adam',
#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
#               metrics=['accuracy'])

# history = model.fit(train_generator,
#                     steps_per_epoch=steps,
#                     validation_data=val_generator,
#                     validation_steps=steps_val,
#                     epochs=200,
#                     verbose=1,
#                     callbacks=[es,mc,lr])

# #%%

import numpy as np

plt.figure(1)
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.yticks(np.arange(0, 1, step=0.05))
plt.grid(color='k', linestyle='--', linewidth=0.4)
plt.legend(loc='lower right')
#plt.savefig(dir + 'accuracy_CNN_ ' + Exp +  '.png')

plt.figure(2)
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label = 'val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
#plt.ylim([0, 1])
#plt.yticks(np.arange(0, 1, step=0.05))
plt.grid(color='k', linestyle='--', linewidth=0.4)
plt.legend(loc='lower right')
#plt.savefig(dir + 'loss_CNN_' + Exp + '.png')

#%%
"""***Evaluation***
---


"""

import math
import albumentations as A
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2
#from tqdm import tqdm # Progress bar

import tensorflow as tf
import tensorflow.keras as keras


#model.load_weights('/home/usuario/Documentos/GBM/Experimentos/PseudPSegmModel.h5')
#model = keras.models.load_model('/home/usuario/Documentos/GBM/Experimentos/PseudPSegmModel.h5')

#%%

path = '/home/usuario/Documentos/GBM/Samples/PC_SegmentationSet/test/PC/PC/'
test_path = '/home/usuario/Documentos/GBM/Samples/PC_SegmentationSet/test/Mask/Mask/'

listfiles = sorted(os.listdir(path))
mask_listfiles = sorted(os.listdir(test_path))

dicescore = []
accuracy = []
sensitivity = []
specificity = []
f1score = []

IoUmetric = []

## Input image to model must be 128x128 therefore 512/4
scale = 4



#for i in range(len(listfiles)):
#for i in tqdm(range(100)):
for i in range(15,16):

  # List of files
  mask_im_name = mask_listfiles[i]
  im_name = listfiles[i]
       
# Groundtruth image (array)
  mask_array=cv2.imread(test_path+mask_im_name)   # Mask image
  im_array = cv2.imread(path+im_name)               # Graylevel image
  im_or = cv2.imread(path+im_name)               # Graylevel image
  
  
  type(im_array)
  #im_gray = im_array.copy()
  im_gray = im_array
  
  #mask_array.shape()
  # Groundtruth mask Image resize
  mask_array=cv2.resize(mask_array,(2048,2048),interpolation = cv2.INTER_AREA)
  
  ## Input image to model must be 128x128 therefore 512/4
  scale = 8
  
  # Image resize must resize (Model input 128 x 128)
  im_array=cv2.resize(im_array,(2048//scale_factor,2048//scale_factor),
                      interpolation = cv2.INTER_AREA)
  im_array=im_array/255
  
  # Adding one dimension to array
  img_array = np.expand_dims(im_array,axis=[0])
  
  # Generate image prediction
  pred_mask = model.predict(img_array)
  
  # Image mask as (NxMx1) array
  pred_mask = pred_mask[0,:,:,0]
  pred_mask = np.uint16(np.round(pred_mask>0.5))
  
  # Resize image to 512x512x1
  pred_mask = cv2.resize(pred_mask,(2048,2048), 
                      interpolation = cv2.INTER_AREA)
  
  true_mask = np.uint16(mask_array[:,:,0])//255
  
  
  
  
  intersectmask = true_mask & pred_mask
  
  #sumintersectmask = np.sum(intersectmask)
  
  sumpredtrue = np.sum(true_mask)+np.sum(pred_mask)
  
  if sumpredtrue != 0:
        
      dice = 2*np.sum(intersectmask)/(np.sum(true_mask)+np.sum(pred_mask)+0.001)
  
      dicescore.append(dice)
  
  true_mask_flat = true_mask.flatten()
  pred_mask_flat = pred_mask.flatten()
  
  p = np.sum(true_mask_flat)
  n = np.sum(np.logical_not(true_mask_flat))
  tp = np.sum(true_mask_flat & pred_mask_flat)
  fp = np.sum(np.logical_not(true_mask_flat) & pred_mask_flat)
  tn = np.sum(np.logical_not(true_mask_flat) & np.logical_not(pred_mask_flat))
  fn = np.sum(true_mask_flat & np.logical_not(pred_mask_flat))
  
  acc = (tp+tn)/(p+n)
  sens = tp/(tp+fn+0.01) # Cuidado BUG!
  spec = tn/(tn+fp)

  IoU = tp/(tp+fp+fn+0.01)

  #f1 = 2*tp/(2*tp+fp+fn)
  
  IoUmetric.append(IoU)
  accuracy.append(acc)
  sensitivity.append(sens)
  specificity.append(spec)
    #f1score.append(f1)

# Metrics

dicescore = np.array(dicescore)
meandice = np.mean(dicescore)
stddice = np.std(dicescore)

accuracy = np.array(accuracy)
meanacc = np.mean(accuracy)
stdacc = np.std(accuracy)

IoU = np.array(IoUmetric)
meanIoU = np.mean(IoUmetric)
stdIoU = np.std(IoUmetric)

# f1sco = np.array(f1score)
# meanf1 = np.mean(f1sco)
# stdf1 = np.std(f1sco)

print('------------------------')    
print('Mean Dice: '+str(meandice))
print('Std Dice: '+str(stddice))
print('------------------------')
print('Mean Acc: '+str(meanacc))
print('Std Acc: '+str(stdacc))
print('------------------------')
print('------------------------')
print('Mean IoU: '+str(meanIoU))
print('Std IoU: '+str(stdIoU))
print('------------------------')

#%%
plt.show()
plt.subplot(1,3,1)
plt.imshow(im_or/255)
plt.axis('off')
plt.title('patch')
plt.subplot(1,3,2)
plt.imshow(true_mask,cmap='gray')
plt.axis('off')
plt.title('ground truth')
plt.subplot(1,3,3) 
plt.imshow(pred_mask,cmap='gray')
plt.title('prediction')
plt.axis('off')
