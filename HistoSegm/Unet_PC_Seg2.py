# -*- coding: utf-8 -*-
"""LungInfSegm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/asandino87/LungCT/blob/v4.8/LungInfSegm.ipynb
"""

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras import Input,layers, models
from tensorflow.keras.layers import Conv2DTranspose,Dropout,Conv2D,BatchNormalization, Activation,MaxPooling2D
from tensorflow.keras import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler

import math
import albumentations as A
import matplotlib.pyplot as plt

import tensorflow as tf
tf.test.gpu_device_name()

import numpy as np
#%%
"""**Experimental setup**"""

experiment_name = 'LungInf_SF2_Filt64_25052021'
scale_factor = 8
filters = 16
image_size = 2048

input_dir = '/home/usuario/Descargas/destino 8/Training/PC/'
mask_dir = '/home/usuario/Descargas/destino 8/Training/PC_SG2/'

val_dir = '/home/usuario/Descargas/destino 8/Validation/PC/'
mask_val_dir = '/home/usuario/Descargas/destino 8/Validation/PC_SG2/'

target_size=(image_size//scale_factor, image_size//scale_factor)

image_datagen = ImageDataGenerator(
                                  rescale=1./255,
                                  # rotation_range=5,                                  
                                  # #fill_mode='nearest',
                                  # #zoom_range=0.2,
                                  # #width_shift_range=0.01,
                                  # #height_shift_range=0.01,
                                   horizontal_flip=True,
                                   vertical_flip=True,
                                   channel_shift_range=0.1,
                                  #validation_split=0.1,
                                  
                                  )

mask_datagen = ImageDataGenerator(rescale=1./255,)
                                  #validation_split=0.1)

image_datagen_val = ImageDataGenerator(rescale=1./255,)
                                   #    validation_split=0.1)

mask_datagen_val = ImageDataGenerator(rescale=1./255,)
                                   #  validation_split=0.1)

image_generator = image_datagen.flow_from_directory(
    input_dir,
    class_mode=None, target_size=target_size,  batch_size=4,
    seed=1, 
    #subset='training'   
    )

mask_generator = image_datagen.flow_from_directory(
    mask_dir,
    class_mode=None, target_size=target_size, batch_size=4,
    seed=1, 
    #subset='training'
    )

image_generator_val = image_datagen_val.flow_from_directory(
    val_dir,
    class_mode=None, target_size=target_size, batch_size=4,
    seed=1, 
    #subset='validation'
    )

mask_generator_val = mask_datagen_val.flow_from_directory(
    mask_val_dir,
    class_mode=None, target_size=target_size, batch_size=4,
    seed=1, 
    #subset='validation'
    )

#%%
steps = image_generator.n//image_generator.batch_size
steps_val = image_generator_val.n//image_generator_val.batch_size


train_generator = zip(image_generator, mask_generator)
val_generator = zip(image_generator_val, mask_generator_val)

#%%

for _ in range(24,25):
    img = image_generator_val.next()
    mask = mask_generator_val.next()
    
    print(img.shape)
    plt.figure(1)
    plt.subplot(1,2,1)
    plt.axis('off')
    plt.imshow(img[0])
    plt.subplot(1,2,2)
    plt.imshow(mask[0])
    plt.axis('off')
    plt.show()
    
#%%


def conv_block(tensor, nfilters, size=3, padding='same', initializer="he_normal"):
    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(tensor)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    return x


def deconv_block(tensor, residual, nfilters, size=3, padding='same', strides=(2, 2)):
    y = Conv2DTranspose(nfilters, kernel_size=(size, size), strides=strides, padding=padding)(tensor)
    y = tf.concat([y, residual], axis=3)
    y = conv_block(y, nfilters)
    return y


def Unet(img_height, img_width, nclasses, filters):
# down
    input_layer = Input(shape=(img_height, img_width, 3), name='image_input')
    conv1 = conv_block(input_layer, nfilters=filters)
    conv1_out = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = conv_block(conv1_out, nfilters=filters*2)
    conv2_out = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = conv_block(conv2_out, nfilters=filters*4)
    conv3_out = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = conv_block(conv3_out, nfilters=filters*8)
    conv4_out = MaxPooling2D(pool_size=(2, 2))(conv4)
    conv4_out = Dropout(0.5)(conv4_out)
    conv5 = conv_block(conv4_out, nfilters=filters*16)
    conv5 = Dropout(0.5,name='BOTTLENECK')(conv5)
# up
    deconv6 = deconv_block(conv5, residual=conv4, nfilters=filters*8)
    deconv6 = Dropout(0.5)(deconv6)
    deconv7 = deconv_block(deconv6, residual=conv3, nfilters=filters*4)
    deconv7 = Dropout(0.5)(deconv7) 
    deconv8 = deconv_block(deconv7, residual=conv2, nfilters=filters*2)
    deconv9 = deconv_block(deconv8, residual=conv1, nfilters=filters)
# output
    output_layer = Conv2D(filters=1, kernel_size=(1, 1))(deconv9)
    output_layer = BatchNormalization()(output_layer)
    output_layer = Activation('sigmoid')(output_layer)

    model = Model(inputs=input_layer, outputs=output_layer, name='Unet')
    return model

nclasses=2

model = Unet(image_size//scale_factor, image_size//scale_factor, nclasses, filters)

model.summary()
#%%

from tensorflow.keras import backend as K

def soft_dice(y_pred, y_true):
    import keras.backend as T
    # y_pred is softmax output of shape (num_samples, num_classes)
    # y_true is one hot encoding of target (shape= (num_samples, num_classes))
    #a=0
    intersect = T.sum(y_pred * y_true, 0)
    denominator = T.sum(y_pred, 0) + T.sum(y_true, 0)
    dice_scores = T.constant(1)-T.constant(2) * intersect / (denominator + T.constant(1e-6))
    #print("aaa")
    return dice_scores

def soft_dice_metrics(y_pred, y_true):
    import keras.backend as T
    # y_pred is softmax output of shape (num_samples, num_classes)
    # y_true is one hot encoding of target (shape= (num_samples, num_classes))
    #a=0
    intersect = T.sum(y_pred * y_true, 0)
    denominator = T.sum(y_pred, 0) + T.sum(y_true, 0)
    dice_scores =  T.constant(1)-T.constant(2) * intersect / (denominator + T.constant(1e-6))
    #print("aaa")
    return dice_scores

def IoULoss(y_pred, y_true):
    
    # smooth = T.constant(1e-6)
    # intersection = T.sum(T.dot(y_pred,y_true))
    # total = T.sum(y_true) + T.sum(y_pred)
    # union = total - intersection
    # IoU = (intersection+smooth) / (union+smooth)
    # IoULossVal = T.constant(1)-IoU
    intersect = K.sum(y_pred * y_true, 0)
    total = K.sum(y_pred, 0) + K.sum(y_true, 0)
    union = total - intersect
    IoU = (intersect+ K.constant(1e-6)) / (union + K.constant(1e-6))
    IoULossVal = K.constant(1)-IoU
    
    #IoULossVal = T.constant(1)-T.constant(2) * intersect / (denominator + T.constant(1e-6))    
    
    
    return IoULossVal

def Jaccard_Metric(y_true, y_pred):
    delta = K.constant(1e-6)
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    union = K.sum(y_true_f) + K.sum(y_pred_f) 
    Jaccard_Coef = (intersection + delta) / (union - intersection + delta)
    return Jaccard_Coef 



#%%


def step_decay(epoch):
	initial_lrate = 1e-3
	drop = 0.1
	epochs_drop = 100
	lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
	return lrate

lr = LearningRateScheduler(step_decay)
es = EarlyStopping(patience=1000,mode='min', verbose=1)
checkpoint_path ='/home/usuario/Documentos/GBM/Experimentos/PseudPSegmModel.h5'

mc = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1 , save_best_only=True, mode='min')

model.compile(optimizer='adam',
              #loss=IoULoss,
              loss = IoULoss,
              metrics=[soft_dice_metrics]
              )

#%%

history = model.fit(train_generator,
                    steps_per_epoch=steps,
                    validation_data=val_generator,
                    validation_steps=steps_val,
                    epochs=500,
                    verbose=1,
                    callbacks=[es,mc,lr]
                    )

#%%

import numpy as np

plt.figure(1)
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.yticks(np.arange(0, 1, step=0.05))
plt.grid(color='k', linestyle='--', linewidth=0.4)
plt.legend(loc='lower right')
#plt.savefig(dir + 'accuracy_CNN_ ' + Exp +  '.png')

#%% 

plt.figure(2)
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label = 'val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
#plt.ylim([0, 1])
#plt.yticks(np.arange(0, 1, step=0.05))
plt.grid(color='k', linestyle='--', linewidth=0.4)
plt.legend(loc='lower right')
#plt.savefig(dir + 'loss_CNN_' + Exp + '.png')

#%%
"""***Evaluation***
---


"""

import math
import albumentations as A
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2
#from tqdm import tqdm # Progress bar

import tensorflow as tf
import tensorflow.keras as keras

#model = keras.models.load_model('/home/usuario/Documentos/GBM/Experimentos/PseudPSegmModel.h5')

#%%

path = '/home/usuario/Descargas/destino 8/PC/Testing/'
test_path = '/home/usuario/Descargas/destino 8/PC_SG2/Testing/'

listfiles = sorted(os.listdir(path))
mask_listfiles = sorted(os.listdir(test_path))

dicescore = []
accuracy = []
sensitivity = []
specificity = []
f1score = []

IoUmetric = []

## Input image to model must be 128x128 therefore 512/4
scale = 2

for i in range(10,11):
#for i in tqdm(range(100)):
#for i in range(2,3):

  # List of filesa
  mask_im_name = mask_listfiles[i]
  im_name = listfiles[i]
       
# Groundtruth image (array)
  mask_array=cv2.imread(test_path+mask_im_name)   # Mask image
  im_array = cv2.imread(path+im_name)               # Graylevel image
  
  
  type(im_array)
  #im_gray = im_array.copy()
  im_gray = im_array
  
  #mask_array.shape()
  # Groundtruth mask Image resize
  mask_array=cv2.resize(mask_array,(2048//8,2048//8),interpolation = cv2.INTER_AREA)
  
  ## Input image to model must be 128x128 therefore 512/4
  scale = 2
  
  # Image resize must resize (Model input 128 x 128)
  im_array=cv2.resize(im_array,(2048//8,2048//8),
                      interpolation = cv2.INTER_AREA)
  
  im_array=im_array/255
  
  # Adding one dimension to array
  img_array = np.expand_dims(im_array,axis=[0])
  
  # Generate image prediction
  pred_mask = model.predict(img_array)
  
  # Image mask as (NxMx1) array
  pred_mask = pred_mask[0,:,:,0]
  pred_mask = np.uint16(np.round(pred_mask))
  
  # Resize image to 512x512x1
  pred_mask = cv2.resize(pred_mask,(1024//4,1024//4), 
                      interpolation = cv2.INTER_AREA)
  
  true_mask = np.uint16(mask_array[:,:,0])//255
  
  plt.show()
  
  plt.subplot(1,3,1)
  plt.imshow(im_array)
  plt.axis('off')
  plt.title('H&E')
  
  plt.subplot(1,3,2)
  plt.imshow(true_mask)
  plt.axis('off')
  plt.title('Grountruth')
  
  plt.subplot(1,3,3)
  plt.imshow(pred_mask)
  plt.axis('off')
  plt.title('Prediction')
  
  intersectmask = true_mask & pred_mask
  
  #sumintersectmask = np.sum(intersectmask)
  
  sumpredtrue = np.sum(true_mask)+np.sum(pred_mask)
  
  if sumpredtrue != 0:
        
      dice = 2*np.sum(intersectmask)/(np.sum(true_mask)+np.sum(pred_mask)+0.001)
  
      dicescore.append(dice)
  
  true_mask_flat = true_mask.flatten()
  pred_mask_flat = pred_mask.flatten()
  
  p = np.sum(true_mask_flat)
  n = np.sum(np.logical_not(true_mask_flat))
  tp = np.sum(true_mask_flat & pred_mask_flat)
  fp = np.sum(np.logical_not(true_mask_flat) & pred_mask_flat)
  tn = np.sum(np.logical_not(true_mask_flat) & np.logical_not(pred_mask_flat))
  fn = np.sum(true_mask_flat & np.logical_not(pred_mask_flat))
  
  acc = (tp+tn)/(p+n)
  sens = tp/(tp+fn+0.01) # Cuidado BUG!
  spec = tn/(tn+fp)

  IoU = tp/(tp+fp+fn+0.01)

  #f1 = 2*tp/(2*tp+fp+fn)
  
  IoUmetric.append(IoU)
  accuracy.append(acc)
  sensitivity.append(sens)
  specificity.append(spec)
    #f1score.append(f1)

# Metrics

dicescore = np.array(dicescore)
meandice = np.mean(dicescore)
stddice = np.std(dicescore)

accuracy = np.array(accuracy)
meanacc = np.mean(accuracy)
stdacc = np.std(accuracy)

IoU = np.array(IoUmetric)
meanIoU = np.mean(IoUmetric)
stdIoU = np.std(IoUmetric)

# f1sco = np.array(f1score)
# meanf1 = np.mean(f1sco)
# stdf1 = np.std(f1sco)

print('------------------------')    
print('Mean Dice: '+str(meandice))
print('Std Dice: '+str(stddice))
print('------------------------')
print('Mean Acc: '+str(meanacc))
print('Std Acc: '+str(stdacc))
print('------------------------')
print('------------------------')
print('Mean IoU: '+str(meanIoU))
print('Std IoU: '+str(stdIoU))
print('------------------------')

